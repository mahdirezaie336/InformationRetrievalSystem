{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ace04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from stopwordsiso import stopwords\n",
    "from positional_index.index import PositionalIndex\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469247c4",
   "metadata": {},
   "source": [
    "## 1 - Loading the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba01854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./IR_data_news_12k.json') as json_file:\n",
    "    docs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['0']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae26d9",
   "metadata": {},
   "source": [
    "## 2 - Doing all preprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3fc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "tokenizer = Tokenizer()\n",
    "stemmer = FindStems()\n",
    "stop_words = stopwords('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1235c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preprocessing ...\n",
      "Finished preprocessing.\n"
     ]
    }
   ],
   "source": [
    "print('Started preprocessing ...')\n",
    "for doc_id in docs:\n",
    "    text = docs[doc_id]['content']\n",
    "    normalized_text = normalizer.normalize(text)            # Normalization\n",
    "    tokens = tokenizer.tokenize_words(normalized_text)      # Tokenization\n",
    "    nonstop_tokens = []                                     # Handling stop words\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            nonstop_tokens.append(token)\n",
    "    stemmed_tokens = pd.Series(nonstop_tokens).apply(stemmer.convert_to_stem).values        # Getting stems\n",
    "    docs[doc_id]['tokens'] = stemmed_tokens\n",
    "print('Finished preprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca73702",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43428eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PositionalIndex()\n",
    "for doc_id in docs:\n",
    "    index.add_from_dict(doc_id, docs[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ea617",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.dictionary['آسیا']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = normalizer.normalize('تحریم های آمریکا ! ایران')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64123188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = tokenizer.tokenize_words(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in ts:\n",
    "    print(stemmer.convert_to_stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c21ac04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textprocessing_venv",
   "language": "python",
   "name": "textprocessing_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
